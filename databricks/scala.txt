What is RDD (Resilient Distributed Dataset)?
RDD (Resilient Distributed Dataset) is a fundamental data structure of Spark and it is the primary data abstraction in Apache Spark and the Spark Core. RDDs are fault-tolerant, immutable distributed collections of objects, which means once you create an RDD you cannot change it. Each dataset in RDD is divided into logical partitions, which can be computed on different nodes of the cluster. 

In other words, RDDs are a collection of objects similar to collections in Scala, with the difference being RDD is computed on several JVMs scattered across multiple physical servers also called nodes in a cluster while a Scala collection lives on a single JVM.

Additionally, RDDs provide data abstraction of partitioning and distribution of the data which designed to run computations in parallel on several nodes, while doing transformations on RDD most of the time we don’t have to worry of the parallelism as Spark by default provides.

This Apache Spark RDD tutorial describes the basic operations available on RDDs, such as map,filter, and persist etc using Scala example. In addition, this tutorial also explains Pair RDD functions which operate on RDDs of key-value pairs such as groupByKey and join etc.

RDD Advantages
– In-Memory Processing
– Immutability
– Fault Tolerance
– Lazy Evolution
– Partitioning
– Parallelize

Limitations
Spark RDDs are not much suitable for applications that make updates to the state store such as storage systems for a web application. For these applications, it is more efficient to use systems that perform traditional update logging and data checkpointing, such as databases. The goal of RDD is to provide an efficient programming model for batch analytics and leave these asynchronous applications.

RDD Creation
RDD’s are created primarily in two different ways, first parallelizing an existing collection and secondly referencing a dataset in an external storage system (HDFS, S3 and many more). 

#####################################################
val spark:SparkSession = SparkSession.builder()
      .master("local[1]")
      .appName("sparkapp1")
      .getOrCreate()    
######################################################
Using sparkContext.parallelize()
sparkContext.parallelize is used to parallelize an existing collection in your driver program. This is a basic method to create RDD and used mainly while POC’s or prototyping and it required all data to be present on the driver program prior to creating RDD hence it is not most used for production applications.

######################################################

//Create RDD from parallelize    
val dataSeq = Seq(("Java", 20000), ("Python", 100000), ("Scala", 3000))   
val rdd=spark.sparkContext.parallelize(dataSeq)
######################################################

For production applications, we mostly create RDD by using external storage systems like HDFS, S3, HBase e.t.c. To make it simple for this Spark tutorial we are using files from the local system and create RDD.

####################################################

